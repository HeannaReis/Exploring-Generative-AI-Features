🧠# 💻 Projeto Desenvolvido no Bootcamp Microsoft Copilot da DIO 🎯

## 🚀 Explorando os Recursos de IA Generativa com Copilot e OpenAI

### Como entregar esse projeto?

1. Crie um novo repositório no GitHub com um nome à sua preferência.
2. Crie uma pasta chamada `inputs` e salve as imagens que você utilizou.
3. Crie uma pasta chamada `output` e salve os resultados de reconhecimento de texto dessas imagens.
4. Crie um arquivo chamado `README.md`, adicione alguns prints, descreva o processo, alguns insights e possibilidades que você aprendeu durante o conteúdo.
5. Compartilhe conosco o link desse repositório através do botão **"Entregar Projeto"**.

### 🔗 Links Importantes:

- [Explore Generative AI with Microsoft Copilot](https://microsoftlearning.github.io/mslearn-ai-fundamentals/Instructions/Labs/12-generative-ai.html)
- [Explore Azure OpenAI](https://microsoftlearning.github.io/mslearn-ai-fundamentals/Instructions/Labs/13-azure-openai.html)
- [Explore content filters in Azure OpenAI](https://microsoftlearning.github.io/mslearn-ai-fundamentals/Instructions/Labs/14-azure-openai-content-filters.html)

📚 **Bons estudos** 😉

---

## 🛠 Solução do Desafio:

### Projeto de IA Generativa Responsável

Este repositório contém a implementação de uma solução de IA generativa responsável, seguindo as diretrizes de **"Identificar"**, **"Medida"**, **"Mitigar"** e **"Operar"**.

### Estrutura do Repositório

- `inputs/`: Contém as imagens utilizadas no projeto.
- `outputs/`: Contém os resultados do reconhecimento de texto nas imagens.
- `README.md`: Este arquivo, descrevendo o processo, insights e possibilidades aprendidas durante o desenvolvimento do projeto.

### 🔄 Processo de Desenvolvimento

#### 1. Identificar 📍
- **Objetivo**: Detectar possíveis riscos e danos relacionados à solução.
  
  **Ações**:
  - **Análise de Impacto**: Mapeamos áreas onde a IA pode causar danos, como discriminação, viés ou geração de conteúdo inadequado.
  - **Envolvimento de Stakeholders**: Consultamos especialistas em ética, engenheiros e usuários finais para coletar perspectivas sobre riscos potenciais.
  - **Revisão de Dados**: Analisamos os dados usados para treinar a IA, identificando padrões que possam gerar vieses.

#### 2. Medida 📏
- **Objetivo**: Quantificar e avaliar os riscos identificados.

  **Ações**:
  - **Métricas de Avaliação**: Desenvolvemos métricas para medir a presença de vieses e outros riscos.
  - **Testes de Estresse**: Realizamos testes para avaliar como a IA se comporta em diferentes cenários, especialmente os mais críticos.
  - **Monitoramento Contínuo**: Implementamos sistemas de monitoramento para detectar problemas em tempo real.

#### 3. Mitigar 🛡️
- **Objetivo**: Reduzir ou eliminar os riscos identificados.

  **Ações**:
  - **Ajuste de Algoritmos**: Modificamos algoritmos para minimizar vieses e outros riscos.
  - **Filtragem de Conteúdo**: Implementamos filtros para evitar a geração de conteúdo inadequado.
  - **Treinamento e Educação**: Capacitamos a equipe sobre práticas éticas e seguras no desenvolvimento de IA.

#### 4. Operar ⚙️
- **Objetivo**: Garantir que a solução de IA funcione de maneira ética e segura no dia a dia.

  **Ações**:
  - **Governança**: Estabelecemos políticas e procedimentos claros para o uso da IA.
  - **Auditorias Regulares**: Realizamos auditorias periódicas para garantir a conformidade com as diretrizes estabelecidas.
  - **Feedback Contínuo**: Coletamos feedback dos usuários para melhorias contínuas.

---

### 📊 Insights e Possibilidades

Durante o desenvolvimento deste projeto, aprendemos a importância de envolver diversos stakeholders para identificar riscos potenciais de maneira abrangente. Além disso, a implementação de métricas e testes de estresse foi crucial para garantir a robustez da solução. A governança contínua e o feedback dos usuários são essenciais para manter a IA operando de maneira ética e segura.

### 🖼️ Prints

**Exemplo de Imagem Utilizada**  
![Exemplo de Imagem](#)

**Resultado do Reconhecimento de Texto**  
![Resultado do Reconhecimento de Texto](#)

---

## 📜 Conclusão

Este projeto demonstra como uma abordagem estruturada e responsável pode ser aplicada no desenvolvimento de soluções de IA generativa. Através das diretrizes de **"Identificar"**, **"Medida"**, **"Mitigar"** e **"Operar"**, conseguimos criar uma solução ética, segura e eficaz.

